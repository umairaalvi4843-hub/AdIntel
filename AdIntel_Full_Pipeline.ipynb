{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+vwk3XDLjm8HDANR9RfdT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umairaalvi4843-hub/AdIntel/blob/main/AdIntel_Full_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8cA-xXlIust",
        "outputId": "6eb07a6f-a082-4af9-cc37-b9b8abb86d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [1/8] Installing libraries ---\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
            "\n",
            "--- [2/8] Mounting Google Drive ---\n",
            "Mounted at /content/drive\n",
            "\n",
            "--- [3/8] Loading data from Drive ---\n",
            "Data loaded. Shape: (3000000, 17)\n",
            "\n",
            "--- [4/8] Creating basic features ---\n",
            "\n",
            "--- [5/8] Splitting data into train and test sets ---\n",
            "Data split complete. Training shape: (2400000, 19) Testing shape: (600000, 19)\n",
            "\n",
            "--- [6/8] Performing Target Encoding ---\n",
            "Feature Engineering complete!\n",
            "\n",
            "--- [7/8] Training high-performance model ---\n",
            "[LightGBM] [Info] Number of positive: 410886, number of negative: 1989114\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.353172 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1099\n",
            "[LightGBM] [Info] Number of data points in the train set: 2400000, number of used features: 17\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171203 -> initscore=-1.577129\n",
            "[LightGBM] [Info] Start training from score -1.577129\n",
            "Model training complete!\n",
            "\n",
            "Evaluating model performance...\n",
            "--- Final Model Performance ---\n",
            "Final LogLoss: 0.4686\n",
            "Final AUC Score: 0.7312\n",
            "\n",
            "--- [8/8] Saving model to file in Google Drive ---\n",
            "Model saved successfully as 'adintel_model.pkl' in your Google Drive.\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 1. SETUP AND IMPORTS\n",
        "# ============================================\n",
        "print(\"--- [1/8] Installing libraries ---\")\n",
        "# Install the correct libraries for this environment\n",
        "!pip install lightgbm joblib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "import joblib\n",
        "import gc\n",
        "from google.colab import drive\n",
        "\n",
        "# ============================================\n",
        "# 2. MOUNT GOOGLE DRIVE\n",
        "# ============================================\n",
        "print(\"\\n--- [2/8] Mounting Google Drive ---\")\n",
        "# This connects your Colab notebook to your Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Drive already mounted.\")\n",
        "\n",
        "# ============================================\n",
        "# 3. LOAD DATA\n",
        "# ============================================\n",
        "print(\"\\n--- [3/8] Loading data from Drive ---\")\n",
        "# Define the path to your data in Google Drive\n",
        "# Make sure to update this path if your folder is named differently\n",
        "file_path = '/content/drive/MyDrive/AdIntel_Data/train.csv'\n",
        "\n",
        "# We load 3 million rows (Colab can handle this)\n",
        "# We also select a smart set of columns for our advanced model\n",
        "cols_to_use = [\n",
        "    'click', 'hour', 'banner_pos', 'site_category', 'app_category',\n",
        "    'device_type', 'device_conn_type', 'device_id', 'C1', 'C14',\n",
        "    'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21'\n",
        "]\n",
        "df = pd.read_csv(file_path, usecols=cols_to_use, nrows=3000000)\n",
        "print(\"Data loaded. Shape:\", df.shape)\n",
        "\n",
        "# ============================================\n",
        "# 4. FEATURE ENGINEERING (BASIC)\n",
        "# ============================================\n",
        "print(\"\\n--- [4/8] Creating basic features ---\")\n",
        "# Sort by time, which is critical for historical features\n",
        "df = df.sort_values('hour')\n",
        "\n",
        "# Create hour_of_day\n",
        "df['hour_of_day'] = df['hour'].astype(str).str[-2:].astype('category') # Optimized\n",
        "\n",
        "# Create user_ad_count (the correct, non-leaky way)\n",
        "df['user_ad_count'] = df.groupby('device_id').cumcount()\n",
        "\n",
        "# ============================================\n",
        "# 5. SPLIT DATA (To prevent data leakage)\n",
        "# ============================================\n",
        "print(\"\\n--- [5/8] Splitting data into train and test sets ---\")\n",
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=42, stratify=df['click']\n",
        ")\n",
        "\n",
        "# Free up memory by deleting the large 'df'\n",
        "del df\n",
        "gc.collect()\n",
        "print(\"Data split complete. Training shape:\", train_df.shape, \"Testing shape:\", test_df.shape)\n",
        "\n",
        "# ============================================\n",
        "# 6. FEATURE ENGINEERING (ADVANCED - TARGET ENCODING)\n",
        "# ============================================\n",
        "print(\"\\n--- [6/8] Performing Target Encoding ---\")\n",
        "target = 'click'\n",
        "# Calculate the overall click rate of our training data\n",
        "global_mean = train_df[target].mean()\n",
        "\n",
        "# Define the features we want to encode\n",
        "high_card_features = [\n",
        "    'banner_pos', 'site_category', 'app_category', 'device_id',\n",
        "    'device_type', 'device_conn_type', 'C1', 'C14', 'C15', 'C16',\n",
        "    'C17', 'C18', 'C19', 'C20', 'C21'\n",
        "]\n",
        "\n",
        "# Create the \"reputation\" map from the training data ONLY\n",
        "encoding_maps = {}\n",
        "for col in high_card_features:\n",
        "    encoding_maps[col] = train_df.groupby(col)[target].mean()\n",
        "\n",
        "# Apply the learned encodings to both training and testing sets\n",
        "def apply_target_encoding(df, encoding_maps, global_mean):\n",
        "    df_encoded = df.copy()\n",
        "    for col, mapping in encoding_maps.items():\n",
        "        new_col_name = f\"{col}_encoded\"\n",
        "        df_encoded[new_col_name] = df_encoded[col].map(mapping)\n",
        "        df_encoded[new_col_name] = df_encoded[new_col_name].fillna(global_mean)\n",
        "    return df_encoded\n",
        "\n",
        "train_df_encoded = apply_target_encoding(train_df, encoding_maps, global_mean)\n",
        "test_df_encoded = apply_target_encoding(test_df, encoding_maps, global_mean)\n",
        "\n",
        "# --- Define Final Training Sets ---\n",
        "features_to_drop = [target, 'hour'] + high_card_features\n",
        "features = [col for col in train_df_encoded.columns if col not in features_to_drop]\n",
        "\n",
        "X_train = train_df_encoded[features]\n",
        "y_train = train_df_encoded[target]\n",
        "X_test = test_df_encoded[features]\n",
        "y_test = test_df_encoded[target]\n",
        "\n",
        "print(\"Feature Engineering complete!\")\n",
        "\n",
        "# ============================================\n",
        "# 7. TRAIN HIGH-PERFORMANCE MODEL\n",
        "# ============================================\n",
        "print(\"\\n--- [7/8] Training high-performance model ---\")\n",
        "model = lgb.LGBMClassifier(\n",
        "    objective='binary',\n",
        "    metric='logloss',\n",
        "    n_estimators=1500,\n",
        "    learning_rate=0.03,\n",
        "    num_leaves=31,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, categorical_feature=['hour_of_day']) # 'hour_of_day' is our only remaining category\n",
        "\n",
        "print(\"Model training complete!\")\n",
        "\n",
        "# --- Evaluate the Model ---\n",
        "print(\"\\nEvaluating model performance...\")\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "loss = log_loss(y_test, y_pred_proba)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"--- Final Model Performance ---\")\n",
        "print(f\"Final LogLoss: {loss:.4f}\")\n",
        "print(f\"Final AUC Score: {auc:.4f}\")\n",
        "\n",
        "# ============================================\n",
        "# 8. SAVE THE FINAL MODEL\n",
        "# ============================================\n",
        "print(\"\\n--- [8/8] Saving model to file in Google Drive ---\")\n",
        "drive_path = '/content/drive/MyDrive/AdIntel_Data/'\n",
        "joblib.dump(model, drive_path + 'adintel_model.pkl')\n",
        "print(\"Model saved successfully as 'adintel_model.pkl' in your Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Define the path to your model file saved in Google Drive\n",
        "model_path = '/content/drive/MyDrive/AdIntel_Data/adintel_model.pkl'\n",
        "\n",
        "# --- 1. Basic Check: File Information ---\n",
        "if os.path.exists(model_path):\n",
        "    print(\"✅ FILE CHECK: Model file found.\")\n",
        "else:\n",
        "    print(\"❌ ERROR: Model file not found. Check your Google Drive path.\")\n",
        "\n",
        "# --- 2. Load and Check Model Parameters ---\n",
        "# Load the model into a variable\n",
        "loaded_model = joblib.load(model_path)\n",
        "\n",
        "# Check the key hyperparameters that show it was the successfully tuned model\n",
        "print(\"\\n--- MODEL DIAGNOSTICS ---\")\n",
        "print(f\"Model Type: {type(loaded_model)}\")\n",
        "print(f\"n_estimators (Trees Built): {loaded_model.n_estimators}\")\n",
        "print(f\"Learning Rate: {loaded_model.learning_rate}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90rSmhPvLVao",
        "outputId": "fc7dbfc2-df51-4281-ec23-779f1abf0e95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FILE CHECK: Model file found.\n",
            "\n",
            "--- MODEL DIAGNOSTICS ---\n",
            "Model Type: <class 'lightgbm.sklearn.LGBMClassifier'>\n",
            "n_estimators (Trees Built): 1500\n",
            "Learning Rate: 0.03\n"
          ]
        }
      ]
    }
  ]
}